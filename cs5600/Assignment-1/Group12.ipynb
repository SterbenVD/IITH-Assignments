{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all csv files in the folder ../data/csvs\n",
    "\n",
    "data_path = '../data/csvs'\n",
    "files = os.listdir(data_path)\n",
    "df = {}\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):\n",
    "        df[file[:-4]] = pd.read_csv(f'{data_path}/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostTypeID = 1 -> Question\n",
    "# PostTypeID = 2 -> Answer\n",
    "answers = df['Posts'][df['Posts']['PostTypeId'] == 2]\n",
    "Answerer_table = answers[['OwnerUserId']].groupby('OwnerUserId').size().reset_index(name='AnswerCount').sort_values(by='AnswerCount', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54136/308645294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  questions['Tags'] = questions['Tags'].fillna('')\n",
      "/tmp/ipykernel_54136/308645294.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  questions['Tags'] = questions['Tags'].apply(get_tags)\n"
     ]
    }
   ],
   "source": [
    "questions = df['Posts'][df['Posts']['PostTypeId'] == 1]\n",
    "\n",
    "# get all unique tags for each question\n",
    "# tagIds are given in 'Tags' dataframe and tags are | separated\n",
    "tags = df['Tags']\n",
    "\n",
    "def get_tags(tag_string):\n",
    "    taglist = tag_string.split('|')\n",
    "    if(len(taglist) > 0):\n",
    "        return taglist[1:-1]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# fill NaN values with empty string\n",
    "questions['Tags'] = questions['Tags'].fillna('')\n",
    "questions['Tags'] = questions['Tags'].apply(get_tags)\n",
    "\n",
    "# each question has a field with the list of tags\n",
    "# for each tag in 'Tags' dataframe, get the count of questions with that tag in 'questions' dataframe\n",
    "def get_tag_count(tag):\n",
    "    return questions['Tags'].apply(lambda y: tag in y).sum()\n",
    "\n",
    "tags['TagCount'] = tags['TagName'].apply(get_tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_table = tags[['Id','TagName', 'TagCount']].sort_values(by='TagCount', ascending=False)\n",
    "tags_table = tags_table.set_index('TagName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all users from the 'Users' dataframe and merge with the answers dataframe\n",
    "users = df['Users']\n",
    "\n",
    "# add users with 0 answers\n",
    "Answerer_table = users.merge(Answerer_table, how='left', left_on='Id', right_on='OwnerUserId')\n",
    "Answerer_table = Answerer_table.fillna(0).sort_values(by='AnswerCount', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Id  AnswerCount    DisplayName\n",
      "53721     9113       2839.0      Doc Brown\n",
      "147175  177980       2326.0           Ewan\n",
      "49017     1204       2043.0  Robert Harvey\n"
     ]
    }
   ],
   "source": [
    "# get the top 3 users with the most answers\n",
    "print(Answerer_table.head(3)[['Id','AnswerCount','DisplayName']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Id  TagCount\n",
      "TagName               \n",
      "design   609      5162\n",
      "c#       249      4931\n",
      "java      76      4928\n"
     ]
    }
   ],
   "source": [
    "# get the top 3 tags with the most questions\n",
    "print(tags_table.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answerer_table = Answerer_table[['Id','AnswerCount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all users with more than 20 answers\n",
    "users_20 = Answerer_table[Answerer_table['AnswerCount'] > 20].sort_values(by='Id', ascending=True)\n",
    "\n",
    "# all tags with more than 20 questions\n",
    "tags_20 = tags[tags['TagCount'] > 20].sort_values(by='Id', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_df = pd.DataFrame(index=users_20['Id'], columns=tags_20['Id']).fillna(0)\n",
    "users_20.set_index('Id', inplace=True)\n",
    "tags_20.set_index('Id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_utility_matrix(ans):\n",
    "    tags = questions[questions['Id'] == ans['ParentId']]['Tags'].iloc[0]\n",
    "    AnswererUserId = ans['OwnerUserId']\n",
    "    if(AnswererUserId not in utility_df.index):\n",
    "        return\n",
    "    tagIdList = []\n",
    "    for tag in tags:\n",
    "        if tag not in tags_table.index:\n",
    "            continue\n",
    "        tagId = tags_table.at[tag,'Id']\n",
    "        if tagId in tags_20.index:\n",
    "            tagIdList.append(tagId)\n",
    "\n",
    "    for tagId in tagIdList:\n",
    "        utility_df.at[AnswererUserId, tagId] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, ans in answers.iterrows():\n",
    "    fill_utility_matrix(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of unknown expert ratings in the utility matrix: 88.89014050660478%\n"
     ]
    }
   ],
   "source": [
    "# count 0s in the utility matrix\n",
    "zero_count = utility_df.apply(lambda x: x == 0).sum().sum()\n",
    "total_count = utility_df.shape[0] * utility_df.shape[1]\n",
    "print(f'Percentage of unknown expert ratings in the utility matrix: {zero_count/total_count*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 0s with NaN as it is unknown\n",
    "utility_df = utility_df.replace(0, float('nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the utility matrix to a numpy array\n",
    "utility_matrix = utility_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the utility matrix: (Users, Tags) (1119, 952)\n"
     ]
    }
   ],
   "source": [
    "# report the dimensions of the utility matrix\n",
    "print(f'Dimensions of the utility matrix: (Users, Tags) {utility_matrix.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the index into a series where the index is the user id and the value is the row number\n",
    "user_index = pd.Series(utility_df.index, index=range(utility_df.shape[0]))\n",
    "\n",
    "# convert the columns into a series where the index is the tag id and the value is the column number\n",
    "tag_index = pd.Series(utility_df.columns, index=range(utility_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4. nan  2. ...  0.  0. nan]\n",
      " [nan nan  2. ... nan nan nan]\n",
      " [ 0. nan  0. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 0. nan nan ... nan  1.  0.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "# for each value that is not NaN convert it to floor(x/3) if x < 15 else 5\n",
    "expert_matrix = np.where(np.isnan(utility_matrix), \n",
    "                          np.nan,  # Keep NaN as it is\n",
    "                          np.where(utility_matrix < 15, utility_matrix // 3, 5))\n",
    "\n",
    "print(expert_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summation of the expert matrix: 41221.0\n",
      "Highest row sum of the expert matrix: 1161.0\n",
      "Highest column sum of the expert matrix: 1394.0\n"
     ]
    }
   ],
   "source": [
    "# report the summation of the expert matrix\n",
    "print(f'Summation of the expert matrix: {np.nansum(expert_matrix)}')\n",
    "\n",
    "# report the highest row sum of the expert matrix\n",
    "print(f'Highest row sum of the expert matrix: {np.nanmax(np.nansum(expert_matrix, axis=1))}')\n",
    "\n",
    "# report the highest column sum of the expert matrix\n",
    "print(f'Highest column sum of the expert matrix: {np.nanmax(np.nansum(expert_matrix, axis=0))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bottom right corner of width 15% of the expert matrix is the test set\n",
    "test_set = np.copy(expert_matrix[int(expert_matrix.shape[0]*0.85):,int(expert_matrix.shape[1]*0.85):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summation of the train set: 40565.0\n",
      "Dimensions of the test set: (168, 143)\n",
      "Summation of the test set: 656.0\n"
     ]
    }
   ],
   "source": [
    "# report the summation of the train set\n",
    "train_sum = np.nansum(expert_matrix) - np.nansum(test_set)\n",
    "print(f'Summation of the train set: {train_sum}')\n",
    "\n",
    "# report the dimensions of the test set\n",
    "print(f'Dimensions of the test set: {test_set.shape}')\n",
    "\n",
    "# report the summation of the test set\n",
    "print(f'Summation of the test set: {np.nansum(test_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: item-item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4., nan,  2., ...,  0.,  0., nan],\n",
       "       [nan, nan,  2., ..., nan, nan, nan],\n",
       "       [ 0., nan,  0., ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [ 0., nan, nan, ..., nan,  1.,  0.],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean of each column and center the values and fill NaN with 0\n",
    "\n",
    "# uncomment this if centering is needed and NaNs should be filled with 0\n",
    "# expert_matrix = expert_matrix - np.nanmean(expert_matrix, axis=0)\n",
    "# expert_matrix = np.nan_to_num(expert_matrix, nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tag similarity matrix has dimensions: (952, 952)\n"
     ]
    }
   ],
   "source": [
    "# Develop an item-item collaborative filtering model using the train set and use pearson correlation for similarity between tags\n",
    "# first we need to calculate the pearson correlation between each pair of tags\n",
    "expert_matrix_df = pd.DataFrame(utility_matrix)\n",
    "tag_correlation = expert_matrix_df.corr()\n",
    "print(f'The tag similarity matrix has dimensions: {tag_correlation.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_prediction_function_item(data,user_index, tag_index, similar_tag_indices):\n",
    "    predicition = 0\n",
    "    non_nan = 0\n",
    "    for i in similar_tag_indices:\n",
    "        if not np.isnan(data[user_index, i]):\n",
    "            predicition += data[user_index, i]\n",
    "            non_nan += 1\n",
    "    if non_nan == 0:\n",
    "        return np.nan\n",
    "    return predicition/non_nan\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_prediction_function_item(data,user_index,tag_index,similar_tag_indices):\n",
    "    predicition = 0\n",
    "    total_weight = 0\n",
    "    for i in similar_tag_indices:\n",
    "        if not np.isnan(data[user_index, i]):\n",
    "            predicition += data[user_index, i] * tag_correlation.at[tag_index, i]\n",
    "            total_weight += tag_correlation.at[tag_index, i]\n",
    "    if total_weight == 0:\n",
    "        return np.nan\n",
    "    return predicition / total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tags(data,user_index,tag_index,tag_correlation,K):\n",
    "    similar_tags = []\n",
    "    # sort the tags similar to the tag_index in descending order into a list of indexes of the tags\n",
    "    similar_tag_indices = tag_correlation[tag_index].sort_values(ascending=False).index\n",
    "    for i in similar_tag_indices:\n",
    "        if i != tag_index and not np.isnan(data[user_index, i]):\n",
    "            similar_tags.append(i)\n",
    "        if len(similar_tags) == K:\n",
    "            break\n",
    "    return similar_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_item_cf(data, K, correlation_matrix,prediction_function):\n",
    "    '''\n",
    "    data: entire dataset of which the bottom right 15% will be used for testing\n",
    "    K: number of neighbors to consider\n",
    "    correlation_matrix: matrix of tag-tag correlation\n",
    "    '''\n",
    "    # deep copy the data\n",
    "    data = np.copy(data)\n",
    "    predictions = np.full((data.shape[0]-int(0.85*data.shape[0]), data.shape[1]-int(0.85*data.shape[1])),np.nan)\n",
    "\n",
    "    # predict for the test set\n",
    "    for i in range(int(0.85*data.shape[0]), data.shape[0]):\n",
    "        for j in range(int(0.85*data.shape[1]), data.shape[1]):\n",
    "            # get the K most similar tags which the user has rated except the tag itself\n",
    "            similar_tag_indices = get_similar_tags(data, i, j, correlation_matrix, K)\n",
    "            # if (K+1) then remove the last element\n",
    "            if len(similar_tag_indices) > K:\n",
    "                similar_tag_indices = similar_tag_indices[:-1]\n",
    "            # get the prediction\n",
    "            predictions[i-int(0.85*data.shape[0]), j-int(0.85*data.shape[1])] = prediction_function(data, i, j, similar_tag_indices)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cf_item_simple_avg_2_neighbours = test_item_cf(expert_matrix, 2, tag_correlation, avg_prediction_function_item)\n",
    "predictions_cf_item_simple_avg_3_neighbours = test_item_cf(expert_matrix, 3, tag_correlation, avg_prediction_function_item)\n",
    "predictions_cf_item_simple_avg_5_neighbours = test_item_cf(expert_matrix, 5, tag_correlation, avg_prediction_function_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cf_item_weighted_avg_2_neighbours = test_item_cf(expert_matrix, 2, tag_correlation, weighted_avg_prediction_function_item)\n",
    "predictions_cf_item_weighted_avg_3_neighbours = test_item_cf(expert_matrix, 3, tag_correlation, weighted_avg_prediction_function_item)\n",
    "predictions_cf_item_weighted_avg_5_neighbours = test_item_cf(expert_matrix, 5, tag_correlation, weighted_avg_prediction_function_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(predictions,test_set):\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    for i in range(predictions.shape[0]):\n",
    "        for j in range(predictions.shape[1]):\n",
    "            if not np.isnan(test_set[i,j]) and not np.isnan(predictions[i,j]):\n",
    "                sum += (predictions[i,j] - test_set[i,j])**2\n",
    "                count += 1\n",
    "    \n",
    "    return np.sqrt(sum/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_simple_avg_2_neighbours = RMSE(predictions_cf_item_simple_avg_2_neighbours, test_set)\n",
    "rmse_simple_avg_3_neighbours = RMSE(predictions_cf_item_simple_avg_3_neighbours, test_set)\n",
    "rmse_simple_avg_5_neighbours = RMSE(predictions_cf_item_simple_avg_5_neighbours, test_set)\n",
    "\n",
    "rmse_weighted_avg_2_neighbours = RMSE(predictions_cf_item_weighted_avg_2_neighbours, test_set)\n",
    "rmse_weighted_avg_3_neighbours = RMSE(predictions_cf_item_weighted_avg_3_neighbours, test_set)\n",
    "rmse_weighted_avg_5_neighbours = RMSE(predictions_cf_item_weighted_avg_5_neighbours, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for simple average method with 2 neighbors: 0.8065539869850512\n",
      "RMSE for simple average method with 3 neighbors: 0.7725755683887762\n",
      "RMSE for simple average method with 5 neighbors: 0.7684676748370861\n",
      "RMSE for weighted average method with 2 neighbors: 0.8002631095407067\n",
      "RMSE for weighted average method with 3 neighbors: 0.7618844545199125\n",
      "RMSE for weighted average method with 5 neighbors: 0.7806460817404667\n"
     ]
    }
   ],
   "source": [
    "# report the RMSE for the simple average method with 2, 3, and 5 neighbors\n",
    "print(f'RMSE for simple average method with 2 neighbors: {rmse_simple_avg_2_neighbours}')\n",
    "print(f'RMSE for simple average method with 3 neighbors: {rmse_simple_avg_3_neighbours}')\n",
    "print(f'RMSE for simple average method with 5 neighbors: {rmse_simple_avg_5_neighbours}')\n",
    "\n",
    "# report the RMSE for the weighted average method with 2, 3, and 5 neighbors\n",
    "print(f'RMSE for weighted average method with 2 neighbors: {rmse_weighted_avg_2_neighbours}')\n",
    "print(f'RMSE for weighted average method with 3 neighbors: {rmse_weighted_avg_3_neighbours}')\n",
    "print(f'RMSE for weighted average method with 5 neighbors: {rmse_weighted_avg_5_neighbours}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: User-User Collaborative System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_matrix = np.where(np.isnan(utility_matrix),\n",
    "                            np.nan,\n",
    "                            np.where(utility_matrix < 15, utility_matrix // 3, 5))\n",
    "\n",
    "# # calculate the mean of each row and center the values and fill NaN with 0\n",
    "# uncomment this if centering is needed and NaNs should be filled with 0\n",
    "# expert_matrix = expert_matrix - np.nanmean(expert_matrix, axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_correlation = expert_matrix_df.T.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_prediction_function_user(data,user_index, tag_index, similar_user_indices):\n",
    "    predicition = 0\n",
    "    non_nan = 0\n",
    "    for i in similar_user_indices:\n",
    "        if not np.isnan(data[i, tag_index]):\n",
    "            predicition += data[i, tag_index]\n",
    "            non_nan += 1\n",
    "    if non_nan == 0:\n",
    "        return np.nan\n",
    "    return predicition/non_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_prediction_function_user(data,user_index,tag_index,similar_user_indices):\n",
    "    predicition = 0\n",
    "    total_weight = 0\n",
    "    for i in similar_user_indices:\n",
    "        if not np.isnan(data[i, tag_index]):\n",
    "            predicition += data[i, tag_index] * user_correlation.at[user_index, i]\n",
    "            total_weight += user_correlation.at[user_index, i]\n",
    "    if total_weight == 0:\n",
    "        return np.nan\n",
    "    return predicition / total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_users(data,user_index,tag_index,user_correlation,K):\n",
    "    similar_users = []\n",
    "    # sort the tags similar to the tag_index in descending order into a list of indexes of the tags\n",
    "    similar_user_indices = user_correlation[user_index].sort_values(ascending=False).index\n",
    "    for i in similar_user_indices:\n",
    "        if i != user_index and not np.isnan(data[i, tag_index]):\n",
    "            similar_users.append(i)\n",
    "        if len(similar_users) == K:\n",
    "            break\n",
    "    return similar_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_user_cf(data, K, correlation_matrix,prediction_function):\n",
    "    '''\n",
    "    data: entire dataset of which the bottom right 15% will be used for testing\n",
    "    K: number of neighbors to consider\n",
    "    correlation_matrix: matrix of tag-tag correlation\n",
    "    '''\n",
    "    # deep copy the data\n",
    "    data = np.copy(data)\n",
    "    predictions = np.full((data.shape[0]-int(0.85*data.shape[0]), data.shape[1]-int(0.85*data.shape[1])),np.nan)\n",
    "\n",
    "    # predict for the test set\n",
    "    for i in range(int(0.85*data.shape[0]), data.shape[0]):\n",
    "        for j in range(int(0.85*data.shape[1]), data.shape[1]):\n",
    "            # get the K most similar tags which the user has rated except the tag itself\n",
    "            similar_user_indices = get_similar_users(data, i, j, user_correlation, K)\n",
    "            # if (K+1) then remove the last element\n",
    "            if len(similar_user_indices) > K:\n",
    "                similar_user_indices = similar_user_indices[:-1]\n",
    "            # get the prediction\n",
    "            predictions[i-int(0.85*data.shape[0]), j-int(0.85*data.shape[1])] = prediction_function(data, i, j, similar_user_indices)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cf_user_simple_avg_2_neighbours = test_user_cf(expert_matrix, 2, user_correlation, avg_prediction_function_user)\n",
    "predictions_cf_user_simple_avg_3_neighbours = test_user_cf(expert_matrix, 3, user_correlation, avg_prediction_function_user)\n",
    "predictions_cf_user_simple_avg_5_neighbours = test_user_cf(expert_matrix, 5, user_correlation, avg_prediction_function_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cf_user_weighted_avg_2_neighbours = test_user_cf(expert_matrix, 2, user_correlation, weighted_avg_prediction_function_user)\n",
    "predictions_cf_user_weighted_avg_3_neighbours = test_user_cf(expert_matrix, 3, user_correlation, weighted_avg_prediction_function_user)\n",
    "predictions_cf_user_weighted_avg_5_neighbours = test_user_cf(expert_matrix, 5, user_correlation, weighted_avg_prediction_function_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the RMSE for the simple average method with 2, 3, and 5 neighbors\n",
    "rmse_simple_avg_2_neighbours_user = RMSE(predictions_cf_user_simple_avg_2_neighbours, test_set)\n",
    "rmse_simple_avg_3_neighbours_user = RMSE(predictions_cf_user_simple_avg_3_neighbours, test_set)\n",
    "rmse_simple_avg_5_neighbours_user = RMSE(predictions_cf_user_simple_avg_5_neighbours, test_set)\n",
    "\n",
    "# report the RMSE for the weighted average method with 2, 3, and 5 neighbors\n",
    "rmse_weighted_avg_2_neighbours_user = RMSE(predictions_cf_user_weighted_avg_2_neighbours, test_set)\n",
    "rmse_weighted_avg_3_neighbours_user = RMSE(predictions_cf_user_weighted_avg_3_neighbours, test_set)\n",
    "rmse_weighted_avg_5_neighbours_user = RMSE(predictions_cf_user_weighted_avg_5_neighbours, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for simple average method with 2 neighbors: 0.9010360159689648\n",
      "RMSE for simple average method with 3 neighbors: 0.8601977707898477\n",
      "RMSE for simple average method with 5 neighbors: 0.8043990213940855\n",
      "RMSE for weighted average method with 2 neighbors: 0.9005418859140297\n",
      "RMSE for weighted average method with 3 neighbors: 0.8586811722400283\n",
      "RMSE for weighted average method with 5 neighbors: 0.8031690767385918\n"
     ]
    }
   ],
   "source": [
    "print(f'RMSE for simple average method with 2 neighbors: {rmse_simple_avg_2_neighbours_user}')\n",
    "print(f'RMSE for simple average method with 3 neighbors: {rmse_simple_avg_3_neighbours_user}')\n",
    "print(f'RMSE for simple average method with 5 neighbors: {rmse_simple_avg_5_neighbours_user}')\n",
    "\n",
    "print(f'RMSE for weighted average method with 2 neighbors: {rmse_weighted_avg_2_neighbours_user}')\n",
    "print(f'RMSE for weighted average method with 3 neighbors: {rmse_weighted_avg_3_neighbours_user}')\n",
    "print(f'RMSE for weighted average method with 5 neighbors: {rmse_weighted_avg_5_neighbours_user}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Latent Factor Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all values to NaN in the test part of the expert matrix\n",
    "expert_matrix[int(expert_matrix.shape[0]*0.85):,int(expert_matrix.shape[1]*0.85):] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the expert matrix from (user x tag) to (tag x user)\n",
    "expert_matrix = expert_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_SVD(k,R):\n",
    "    U, S, VT = np.linalg.svd(R, full_matrices=False)\n",
    "    U_k = U[:, :k] \n",
    "    S_k = np.diag(S[:k])\n",
    "    VT_k = VT[:k, :] \n",
    "\n",
    "    Q = U_k\n",
    "    P = np.dot(VT_k.T, S_k)\n",
    "    return Q, P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient WRT Q\n",
    "def unregularized_gradient_Q_SGD(R, Q, P,user,tag):\n",
    "    # missing values are set to NaN\n",
    "    R_hat = np.dot(Q[tag],P[user])\n",
    "\n",
    "    # clipping if R_hat is too large\n",
    "    R_hat = np.clip(R_hat, -1e10, 1e10)\n",
    "\n",
    "    # Using SSE as the loss function\n",
    "    loss = (R[tag,user] - R_hat)**2\n",
    "\n",
    "    # calculate the gradient (SGD)\n",
    "    gradient = -2 * (R[tag,user] - R_hat) * P[user]\n",
    "    gradient = np.clip(gradient, -1e10, 1e10)\n",
    "    return gradient , loss\n",
    "\n",
    "# calculate gradient WRT P\n",
    "def unregularized_gradient_P_SGD(R, Q, P,user,tag):\n",
    "    # missing values are set to NaN\n",
    "    R_hat = np.dot(Q[tag],P[user])\n",
    "    R_hat = np.clip(R_hat, -1e10, 1e10)\n",
    "\n",
    "    # Using SSE as the loss function\n",
    "    loss = (R[tag,user] - R_hat)**2\n",
    "\n",
    "    # calculate the gradient (SGD)\n",
    "    gradient = -2 * (R[tag,user] - R_hat) * Q[tag]\n",
    "    gradient = np.clip(gradient, -1e10, 1e10)\n",
    "    return gradient , loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(R, Q, P, learning_rate, iterations):\n",
    "    for i in range(iterations):\n",
    "        if i % 10 == 0:\n",
    "            # get average loss\n",
    "            loss = 0\n",
    "            count = 0\n",
    "            for tag in range(R.shape[0]):\n",
    "                for user in range(R.shape[1]):\n",
    "                    if not np.isnan(R[tag,user]):\n",
    "                        loss += (R[tag,user] - np.dot(Q[tag],P[user]))**2\n",
    "                        count += 1\n",
    "            loss /= count\n",
    "            print(f'Iteration: {i}, Loss: {loss}')\n",
    "        for tag in range(R.shape[0]):\n",
    "            for user in range(R.shape[1]):\n",
    "                if not np.isnan(R[tag,user]):\n",
    "                    gradient_Q, loss = unregularized_gradient_Q_SGD(R, Q, P, user, tag)\n",
    "                    gradient_P, loss = unregularized_gradient_P_SGD(R, Q, P, user, tag)\n",
    "\n",
    "                    Q[tag] = Q[tag] - learning_rate * gradient_Q\n",
    "                    P[user] = P[user] - learning_rate * gradient_P\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN (unknown) in test set: 21757\n",
      "Total number of values in test set: 24024\n"
     ]
    }
   ],
   "source": [
    "# get NaN in test set\n",
    "nan_in_test = np.isnan(test_set).sum()\n",
    "\n",
    "print(f'Number of NaN (unknown) in test set: {nan_in_test}')\n",
    "print(f'Total number of values in test set: {test_set.shape[0] * test_set.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss on test set\n",
    "def test_loss(R,Q,P):\n",
    "    loss = 0\n",
    "    count = 0\n",
    "    predictions = np.dot(Q,P.T).T\n",
    "\n",
    "    # get predictions on test set\n",
    "    predictions_test = predictions[int(predictions.shape[0]*0.85):,int(predictions.shape[1]*0.85):]\n",
    "\n",
    "    # calculate loss\n",
    "    for i in range(test_set.shape[0]):\n",
    "        for j in range(test_set.shape[1]):\n",
    "            if not np.isnan(test_set[i,j]):\n",
    "                loss += (test_set[i,j] - predictions_test[i,j])**2\n",
    "                count += 1\n",
    "    return np.sqrt(loss/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unregulated(k, R, learning_rate, iterations):\n",
    "    # initialize Q and P\n",
    "    Q, P = np.random.rand(R.shape[0], k), np.random.rand(R.shape[1], k)\n",
    "\n",
    "    # train the model\n",
    "    Q, P = train(R, Q, P, learning_rate, iterations)\n",
    "\n",
    "    # training loss\n",
    "    print(f'k= {k}, learning_rate= {learning_rate}, loss= {test_loss(R,Q,P)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 0.9426492947150403\n",
      "Iteration: 10, Loss: 0.5866564095733755\n",
      "Iteration: 20, Loss: 0.46011725202741255\n",
      "Iteration: 30, Loss: 0.42126671159936824\n"
     ]
    }
   ],
   "source": [
    "train_unregulated(2,expert_matrix, 0.0005, 50)\n",
    "print()\n",
    "train_unregulated(5,expert_matrix, 0.0005, 50)\n",
    "print()\n",
    "train_unregulated(10,expert_matrix, 0.0005, 50)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_gradient_Q(R, Q, P, user, tag, lambda_):\n",
    "    R_hat = np.dot(Q[tag],P[user])\n",
    "    R_hat = np.clip(R_hat, -1e10, 1e10)\n",
    "\n",
    "    loss = (R[tag,user] - R_hat)**2\n",
    "\n",
    "    gradient = -2 * (R[tag,user] - R_hat) * P[user] + 2 * lambda_ * Q[tag]\n",
    "    gradient = np.clip(gradient, -1e10, 1e10)\n",
    "    return gradient , loss\n",
    "\n",
    "def regularized_gradient_P(R, Q, P, user, tag, lambda_):\n",
    "    R_hat = np.dot(Q[tag],P[user])\n",
    "    R_hat = np.clip(R_hat, -1e10, 1e10)\n",
    "\n",
    "    loss = (R[tag,user] - R_hat)**2\n",
    "\n",
    "    gradient = -2 * (R[tag,user] - R_hat) * Q[tag] + 2 * lambda_ * P[user]\n",
    "    gradient = np.clip(gradient, -1e10, 1e10)\n",
    "    return gradient , loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(R,Q,P,learning_rate,iterations,lambda_P,lambda_Q):\n",
    "    for i in range(iterations):\n",
    "        if i % 10 == 0:\n",
    "            loss = 0\n",
    "            count = 0\n",
    "            for tag in range(R.shape[0]):\n",
    "                for user in range(R.shape[1]):\n",
    "                    if not np.isnan(R[tag,user]):\n",
    "                        loss += (R[tag,user] - np.dot(Q[tag],P[user]))**2\n",
    "                        count += 1\n",
    "            loss /= count\n",
    "            print(f'Iteration: {i}, Loss: {loss}')\n",
    "        for tag in range(R.shape[0]):\n",
    "            for user in range(R.shape[1]):\n",
    "                if not np.isnan(R[tag,user]):\n",
    "                    gradient_Q, loss = regularized_gradient_Q(R, Q, P, user, tag, lambda_Q)\n",
    "                    gradient_P, loss = regularized_gradient_P(R, Q, P, user, tag, lambda_P)\n",
    "\n",
    "                    Q[tag] = Q[tag] - learning_rate * gradient_Q\n",
    "                    P[user] = P[user] - learning_rate * gradient_P\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regularised(k,lambda_p, lambda_q, R, learning_rate, iterations):\n",
    "    Q, P = np.random.rand(R.shape[0], k), np.random.rand(R.shape[1], k)\n",
    "\n",
    "    Q, P = train(R, Q, P, learning_rate, iterations,lambda_p,lambda_q)\n",
    "\n",
    "    print(f'k= {k}, lambda_p= {lambda_p}, lambda_q= {lambda_q}, learning_rate= {learning_rate}, loss= {test_loss(R,Q,P)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 0.9224288757818738\n",
      "Iteration: 10, Loss: 0.5903919535634636\n",
      "Iteration: 20, Loss: 0.4625333542580428\n",
      "Iteration: 30, Loss: 0.42187946611514165\n",
      "Iteration: 40, Loss: 0.40987166659824925\n",
      "k= 2, lambda_p= 0.001, lambda_q= 0.003, learning_rate= 0.0005, loss= 0.6915050322458568\n",
      "\n",
      "Iteration: 0, Loss: 0.9353437772724426\n",
      "Iteration: 10, Loss: 0.6100040155844068\n",
      "Iteration: 20, Loss: 0.4818225258842947\n",
      "Iteration: 30, Loss: 0.43544300878923997\n",
      "Iteration: 40, Loss: 0.42056819435171205\n",
      "k= 2, lambda_p= 0.05, lambda_q= 0.05, learning_rate= 0.0005, loss= 0.6922477782971894\n",
      "\n",
      "Iteration: 0, Loss: 0.9405706856229342\n",
      "Iteration: 10, Loss: 0.8572181412790476\n",
      "Iteration: 20, Loss: 0.8800492587711841\n",
      "Iteration: 30, Loss: 0.8937229287543261\n",
      "Iteration: 40, Loss: 0.9030358991185512\n",
      "k= 2, lambda_p= 0.5, lambda_q= 0.75, learning_rate= 0.0005, loss= 0.8325892024456546\n",
      "\n",
      "Iteration: 0, Loss: 1.8592671379468941\n",
      "Iteration: 10, Loss: 0.7632153643060868\n",
      "Iteration: 20, Loss: 0.5688930219448506\n",
      "Iteration: 30, Loss: 0.45574001090870137\n",
      "Iteration: 40, Loss: 0.41465494733428526\n",
      "k= 5, lambda_p= 0.001, lambda_q= 0.003, learning_rate= 0.0005, loss= 0.726756304519257\n",
      "\n",
      "Iteration: 0, Loss: 1.8823525984245102\n",
      "Iteration: 10, Loss: 0.7713578137815729\n",
      "Iteration: 20, Loss: 0.5957426435989477\n",
      "Iteration: 30, Loss: 0.4804489251678644\n",
      "Iteration: 40, Loss: 0.43210423671586884\n",
      "k= 5, lambda_p= 0.05, lambda_q= 0.05, learning_rate= 0.0005, loss= 0.7426751885531632\n",
      "\n",
      "Iteration: 0, Loss: 1.9217502359501002\n",
      "Iteration: 10, Loss: 0.8708682588281457\n",
      "Iteration: 20, Loss: 0.8822005340583245\n",
      "Iteration: 30, Loss: 0.8929577260587866\n",
      "Iteration: 40, Loss: 0.9010968043600831\n",
      "k= 5, lambda_p= 0.5, lambda_q= 0.75, learning_rate= 0.0005, loss= 0.8290046999355295\n",
      "\n",
      "Iteration: 0, Loss: 6.028218722144439\n",
      "Iteration: 10, Loss: 0.9282538895794555\n",
      "Iteration: 20, Loss: 0.6375164103530451\n",
      "Iteration: 30, Loss: 0.4524145440297217\n",
      "Iteration: 40, Loss: 0.4073824245306999\n"
     ]
    }
   ],
   "source": [
    "K = [2,5,10]\n",
    "lambdas = [(0.001,0.003),(0.05,0.05),(0.5,0.75)]\n",
    "\n",
    "for k in K:\n",
    "    for lambda_p, lambda_q in lambdas:\n",
    "        train_regularised(k, lambda_p, lambda_q, expert_matrix, 0.0005, 50)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Surprise Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBaseline, Reader, Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_matrix_df = pd.DataFrame(expert_matrix.T,index=user_index,columns=tag_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_matrix_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted = expert_matrix_df.reset_index().melt(id_vars='Id',var_name='tag',value_name='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glabal_mean = df_melted['rating'].mean()\n",
    "glabal_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the global mean to fill NaN values\n",
    "df_melted['rating'].fillna(glabal_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options_item = {\n",
    "    'name': 'pearson',\n",
    "    'user_based': False,\n",
    "}\n",
    "\n",
    "sim_options_user = {\n",
    "    'name': 'pearson',\n",
    "    'user_based': True,\n",
    "}\n",
    "\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "data = Dataset.load_from_df(df_melted[['Id','tag','rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_2_item = KNNBaseline(k=2, min_k=1, sim_options=sim_options_item)\n",
    "algo_3_item = KNNBaseline(k=3, min_k=1, sim_options=sim_options_item)\n",
    "algo_5_item = KNNBaseline(k=5, min_k=1, sim_options=sim_options_item)\n",
    "algo_2_user = KNNBaseline(k=2, min_k=1, sim_options=sim_options_user)\n",
    "algo_3_user = KNNBaseline(k=3, min_k=1, sim_options=sim_options_user)\n",
    "algo_5_user = KNNBaseline(k=5, min_k=1, sim_options=sim_options_user)\n",
    "\n",
    "algo_2_item.fit(data.build_full_trainset())\n",
    "algo_3_item.fit(data.build_full_trainset())\n",
    "algo_5_item.fit(data.build_full_trainset())\n",
    "algo_2_user.fit(data.build_full_trainset())\n",
    "algo_3_user.fit(data.build_full_trainset())\n",
    "algo_5_user.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_matrix = expert_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the test set to a list of tuples\n",
    "test_set_2 = expert_matrix[int(expert_matrix.shape[0]*0.85):,int(expert_matrix.shape[1]*0.85):]\n",
    "test_set_list = []\n",
    "for i in range(test_set_2.shape[0]):\n",
    "    for j in range(test_set_2.shape[1]):\n",
    "        if not np.isnan(test_set[i,j]):\n",
    "            test_set_list.append((user_index[i],tag_index[j],test_set[i,j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2_item = algo_2_item.test(test_set_list)\n",
    "predictions_3_item = algo_3_item.test(test_set_list)\n",
    "predictions_5_item = algo_5_item.test(test_set_list)\n",
    "predictions_2_user = algo_2_user.test(test_set_list)\n",
    "predictions_3_user = algo_3_user.test(test_set_list)\n",
    "predictions_5_user = algo_5_user.test(test_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'RMSE for 2 neighbours item based: {accuracy.rmse(predictions_2_item)}')\n",
    "print(f'RMSE for 3 neighbours item based: {accuracy.rmse(predictions_3_item)}')\n",
    "print(f'RMSE for 5 neighbours item based: {accuracy.rmse(predictions_5_item)}')\n",
    "print(f'RMSE for 2 neighbours user based: {accuracy.rmse(predictions_2_user)}')\n",
    "print(f'RMSE for 3 neighbours user based: {accuracy.rmse(predictions_3_user)}')\n",
    "print(f'RMSE for 5 neighbours user based: {accuracy.rmse(predictions_5_user)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: SVD based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_factors': [2, 5, 10], 'n_epochs': [10], 'lr_all': [0.002, 0.005], 'reg_all': [0.02, 0.05]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=5)\n",
    "data_gs = Dataset.load_from_df(df_melted[['Id','tag','rating']], reader)\n",
    "gs.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = gs.best_params['rmse']\n",
    "print(\"Best parameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVD\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "best_svd = SVD(n_factors=best_params['n_factors'], n_epochs=best_params['n_epochs'], lr_all=best_params['lr_all'], reg_all=best_params['reg_all'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best SVD: n_factors=10, n_epochs=10, lr_all=0.005, reg_all=0.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svd.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svd = best_svd.test(test_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'RMSE for best SVD: {accuracy.rmse(predictions_svd)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_2 = SVD(n_factors=2, n_epochs=10, lr_all=0.005, reg_all=0.02)\n",
    "svd_5 = SVD(n_factors=5, n_epochs=10, lr_all=0.005, reg_all=0.02)\n",
    "\n",
    "svd_2.fit(data.build_full_trainset())\n",
    "svd_5.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svd_2 = svd_2.test(test_set_list)\n",
    "predictions_svd_5 = svd_5.test(test_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'RMSE for SVD with 2 factors: {accuracy.rmse(predictions_svd_2)}')\n",
    "print(f'RMSE for SVD with 5 factors: {accuracy.rmse(predictions_svd_5)}')\n",
    "print(f'RMSE for SVD with 10 factors: {accuracy.rmse(predictions_svd)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
