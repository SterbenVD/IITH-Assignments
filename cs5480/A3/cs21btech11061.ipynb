{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HahnfGw5espM"
      },
      "source": [
        "# Deep Learning Assignment 3\n",
        "## Vishal Vijay Devadiga\n",
        "## CS21BTECH11061\n",
        "\n",
        "Trained in Colab with T4 GPU\n",
        "\n",
        "# Instructions\n",
        "\n",
        "- Answer all questions. We encourage best coding practices by not penalizing (i.e. you may not get full marks if you make it difficult for us to understand. Hence, use intuitive names for the variables, and comment your code liberally. You may use the text cells in the notebook for briefly explaining the objective of a code cell.)\n",
        "- It is expected that you work on these problems individually. If you have any doubts please contact the TA or the instructor no later than 2 days prior to the deadline.\n",
        "- You may use built-in implementations only for the basic functions such as sqrt, log, etc. from libraries such as numpy or PyTorch. Other high-level functionalities are expected to be implemented by the students. (Individual problem statements will make this clear. We can use the optimizers\n",
        "provided by the libraries such as PyTorch.)\n",
        "- For plots, you may use matplotlib and generate clear plots that are complete and easy to understand.\n",
        "- You are expected to submit the Python Notebooks saved as <your-roll-number>.ipynb\n",
        "- If you are asked to report your observations, use the mark down text cells in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qf7rls2EespN"
      },
      "outputs": [],
      "source": [
        "# All imports and global variables\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision as tv\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Whether to use stochastic gradient descent or not\n",
        "sgd = False\n",
        "\n",
        "set_batch_set = 64\n",
        "if sgd:\n",
        "    set_batch_set = 1\n",
        "\n",
        "# Number of epochs\n",
        "number_of_epoch = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiUhaG9XespO",
        "outputId": "615631ae-acf6-4613-e6a1-7b00b4ace7c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set device: GPU or CPU. Use GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CipXP1xhespO",
        "outputId": "ed010bbc-5324-42c9-db2b-530cba8283e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Import CIFAR-10 dataset from torchvision\n",
        "\n",
        "tr = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                         , transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5), transforms.RandomRotation(30)\n",
        "                         ])\n",
        "\n",
        "train_set = tv.datasets.CIFAR10(root='./cifar', train=True, download=True, transform=tr)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=set_batch_set, shuffle=True, num_workers=2)\n",
        "test_set = tv.datasets.CIFAR10(root='./cifar', train=False, download=True, transform=tr)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=set_batch_set, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ody1mIy7espP"
      },
      "source": [
        "# Question 1: Self-Attention for Object Recognition with CNNs\n",
        "\n",
        "Implement a sample CNN with one or more self-attention layer(s) for performing object recognition over CIFAR-10 dataset.\n",
        "You have to implement the self-attention layer yourself and use it in the forward function defined by you.\n",
        "All other layers (fully connected, nonlinearity, conv layer, etc.) can be bulit-in implementations.\n",
        "The network can be a simpler one (e.g., it may have 1x Conv, 4x [Conv followed by SA], 1x Conv, and 1x GAP).\n",
        "\n",
        "[10 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q-vdR9J-espP"
      },
      "outputs": [],
      "source": [
        "# Define the Self Attention Layer\n",
        "class SelfAtten(nn.Module):\n",
        "    # Self Attention Layer Initialization\n",
        "    def __init__(self, in_dim, stride = 8):\n",
        "        # stride is the factor by which the input dimension is reduced\n",
        "        # Here, default is 8, which means the input dimension is reduced by 8x\n",
        "        # in_dim is the input dimension of the input tensor\n",
        "\n",
        "        # Call the parent class constructor\n",
        "        super(SelfAtten, self).__init__()\n",
        "\n",
        "        # Initialize all required parameters\n",
        "        self.in_dim = in_dim\n",
        "        self.stride = stride\n",
        "\n",
        "        # Check if in_dim is divisible by stride\n",
        "        if in_dim % stride != 0:\n",
        "            raise ValueError(\"in_dim must be divisible by stride\")\n",
        "\n",
        "        self.layer_query = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//stride, kernel_size= 1)\n",
        "        self.layer_key = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//stride, kernel_size= 1)\n",
        "        self.layer_value = nn.Conv2d(in_channels = in_dim , out_channels = in_dim, kernel_size= 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax  = nn.Softmax(dim=-1)\n",
        "\n",
        "    # Forward pass of the self attention layer\n",
        "    def forward(self,x):\n",
        "        # x is the input tensor\n",
        "        # x has shape (batch_size, in_dim, width, height)\n",
        "        # batch_size is the number of samples in the batch\n",
        "        # in_dim is the input dimension of the input tensor\n",
        "        # width is the width of the input tensor\n",
        "        # height is the height of the input tensor\n",
        "        batch_size = x.size(0)\n",
        "        chan = x.size(1)\n",
        "        width = x.size(2)\n",
        "        height = x.size(3)\n",
        "\n",
        "        # Project the input tensor to query, key and value tensors\n",
        "        proj_query = self.layer_query(x).view(batch_size, -1, width*height).permute(0,2,1)\n",
        "        proj_key = self.layer_key(x).view(batch_size, -1, width*height)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        atten = self.softmax(energy)\n",
        "        proj_value = self.layer_value(x).view(batch_size, -1, width*height)\n",
        "\n",
        "        # Compute the output tensor\n",
        "        out = torch.bmm(proj_value, atten.permute(0,2,1))\n",
        "        out = out.view(batch_size, chan, width, height)\n",
        "\n",
        "        # Apply the gamma parameter\n",
        "        out = self.gamma*out + x\n",
        "\n",
        "        # Return the output tensor\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L1XiMZrPespQ"
      },
      "outputs": [],
      "source": [
        "# Define the Self Attention CNN\n",
        "class SelfAttenCNN(nn.Module):\n",
        "    # Self Attention CNN Initialization\n",
        "    def __init__(self, layers, out_activation = nn.Softmax()):\n",
        "        # layers is a list of layer objects\n",
        "        # activation is the activation function to be used\n",
        "        super(SelfAttenCNN, self).__init__()\n",
        "        self.layers = len(layers)\n",
        "\n",
        "        # Create the layer list\n",
        "        self.layer_list = nn.ModuleList()\n",
        "        for i in range(self.layers):\n",
        "            self.layer_list.append(layers[i])\n",
        "\n",
        "        # Add the activation function\n",
        "        self.out_activation = out_activation\n",
        "\n",
        "    # Forward pass of the Self Attention CNN\n",
        "    def forward(self, x):\n",
        "        # x is the input tensor\n",
        "        # x has shape (batch_size, in_dim, width, height)\n",
        "        # batch_size is the number of samples in the batch\n",
        "        # in_dim is the input dimension of the input tensor\n",
        "        # width is the width of the input tensor\n",
        "        # height is the height of the input tensor\n",
        "        for i, l in enumerate(self.layer_list):\n",
        "            x = l(x)\n",
        "        x = self.out_activation(x)\n",
        "        return x\n",
        "\n",
        "    # Run the model\n",
        "    def run_model(self, criterion, optimizer, num_epoch, train_loader):\n",
        "        # criterion is the loss function\n",
        "        # optimizer is the optimizer\n",
        "        # num_epoch is the number of epochs\n",
        "        # train_loader is the training data loader\n",
        "        # logs is a boolean variable to print logs\n",
        "\n",
        "        # For each epoch\n",
        "        for epoch in range(num_epoch):\n",
        "            # Set the model to training mode\n",
        "            self.train()\n",
        "\n",
        "            # Find the batch size\n",
        "            batch_size = train_loader.batch_size\n",
        "\n",
        "            for data in tqdm(train_loader, total = len(train_loader)):\n",
        "\n",
        "                # Get the inputs and labels\n",
        "                inputs = data[0]\n",
        "                labels = data[1]\n",
        "\n",
        "                # If number of samples in the batch is less than the batch size, continue\n",
        "                if inputs.size()[0] != batch_size:\n",
        "                    continue\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self(inputs.to(device))\n",
        "\n",
        "                # Compute the loss\n",
        "                loss = criterion(outputs, labels.to(device))\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "\n",
        "                # Optimize\n",
        "                optimizer.step()\n",
        "\n",
        "    # Test the model\n",
        "    def test_model(self, test_loader):\n",
        "        corr = 0\n",
        "        tot = 0\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in tqdm(test_loader, total = len(test_loader)):\n",
        "                images = data[0]\n",
        "                labels = data[1]\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = self(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                tot += labels.size(0)\n",
        "                corr += (predicted == labels).sum().item()\n",
        "        acc = corr/tot\n",
        "        return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJwWQNx-espQ",
        "outputId": "63e0e26d-6c70-45dc-f718-ae21fc8c5188"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/782 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "100%|██████████| 782/782 [01:36<00:00,  8.13it/s]\n",
            "100%|██████████| 782/782 [01:35<00:00,  8.23it/s]\n",
            "100%|██████████| 782/782 [01:34<00:00,  8.24it/s]\n",
            "100%|██████████| 782/782 [01:34<00:00,  8.24it/s]\n",
            "100%|██████████| 782/782 [01:34<00:00,  8.24it/s]\n",
            "100%|██████████| 782/782 [01:34<00:00,  8.24it/s]\n",
            "100%|██████████| 782/782 [01:34<00:00,  8.24it/s]\n",
            "100%|██████████| 782/782 [01:34<00:00,  8.24it/s]\n",
            "100%|██████████| 782/782 [01:34<00:00,  8.24it/s]\n",
            "100%|██████████| 782/782 [01:34<00:00,  8.24it/s]\n",
            "100%|██████████| 157/157 [00:05<00:00, 27.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.5386\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the layers\n",
        "\n",
        "layers = [\n",
        "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    SelfAtten(32),\n",
        "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    SelfAtten(64),\n",
        "    nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    SelfAtten(128),\n",
        "    nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AvgPool2d(8),\n",
        "    nn.Conv2d(256, 10, kernel_size=1),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(160,10)\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "\n",
        "model = SelfAttenCNN(layers)\n",
        "model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "if sgd:\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Run the model\n",
        "\n",
        "model.run_model(criterion, optimizer, number_of_epoch, train_loader)\n",
        "\n",
        "# Test the model\n",
        "\n",
        "acc = model.test_model(test_loader)\n",
        "\n",
        "print(\"Accuracy: \", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbLXZ_CYespQ"
      },
      "source": [
        "# Question 2: Object Recognition with Vision Transformer\n",
        "\n",
        "Implement and train an Encoder only Transformer (ViT-like) for the above object recognition task.\n",
        "In other words, implement multi-headed self-attention for the image classification (i.e., appending a <class>token to the image patches that are accepted as input tokens).\n",
        "Compare the performance of the two implementations (try to keep the number of parameters to be comparable and use the same amount of training and testing data).\n",
        "\n",
        "[10 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hdTNyZ3ZespQ"
      },
      "outputs": [],
      "source": [
        "# Define the Multi Head Attention Layer\n",
        "class MultHeadAtten(nn.Module):\n",
        "    # Multi Head Attention Layer Initialization\n",
        "    def __init__(self, emb_dim, num_heads = 8):\n",
        "        # emb_dim is the input dimension of the input tensor\n",
        "        # num_heads is the number of heads in the multi head attention layer\n",
        "\n",
        "        # Call the parent class constructor\n",
        "        super(MultHeadAtten, self).__init__()\n",
        "\n",
        "        # Check if the embedding dimension is divisible by the number of heads\n",
        "        if emb_dim % num_heads != 0:\n",
        "            raise ValueError(\"Embedding dimension must be divisible by number of heads\")\n",
        "\n",
        "        # Initialize all required parameters\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = emb_dim // num_heads\n",
        "\n",
        "        # Initialize the query, key and value layers\n",
        "        self.layer_query = nn.Linear(emb_dim, emb_dim)\n",
        "        self.layer_key = nn.Linear(emb_dim, emb_dim)\n",
        "        self.layer_value = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "        # Initialize the output layer\n",
        "        self.layer_out = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    # Forward pass of the multi head attention layer\n",
        "    def forward(self, q, k, v):\n",
        "        # q is the query tensor\n",
        "        # k is the key tensor\n",
        "        # v is the value tensor\n",
        "        # q, k and v have shape (batch_size, emb_dim, width, height)\n",
        "\n",
        "        batch_size = q.shape[0]\n",
        "\n",
        "        # Project the query, key and value tensors\n",
        "        Q = self.layer_query(q)\n",
        "        K = self.layer_key(k)\n",
        "        V = self.layer_value(v)\n",
        "\n",
        "        # Split the query, key and value tensors into multiple heads\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Compute the energy tensor\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / torch.tensor(self.head_dim ** 0.5)\n",
        "\n",
        "        # Compute the attention tensor\n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "\n",
        "        # Compute the output tensor\n",
        "        out = torch.matmul(attention, V)\n",
        "\n",
        "        # Combine the heads\n",
        "        out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.emb_dim)\n",
        "\n",
        "        # Apply the output layer\n",
        "        out = self.layer_out(out)\n",
        "\n",
        "        # Return the output tensor and the attention tensor\n",
        "        return out, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CnAI-xZqisES"
      },
      "outputs": [],
      "source": [
        "# Define the Attention Block\n",
        "class AttenBlock(nn.Module):\n",
        "    # Attention Block Initialization\n",
        "    def __init__(self, emb_dim, hid_dim, num_heads = 8, drop_out = 0.0):\n",
        "        # emb_dim is the input dimension of the input tensor\n",
        "        # hid_dim is the hidden dimension of the input tensor\n",
        "        # num_heads is the number of heads in the multi head attention layer\n",
        "        # drop_out is the drop out rate\n",
        "\n",
        "        # Call the parent class constructor\n",
        "        super(AttenBlock, self).__init__()\n",
        "\n",
        "        # Initialize the multi head attention layer\n",
        "        self.multi_head_atten = MultHeadAtten(emb_dim, num_heads)\n",
        "\n",
        "        # Initialize the layer normalization\n",
        "        self.layer_pre_norm = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm = nn.LayerNorm(emb_dim)\n",
        "\n",
        "        # Initialize the feed forward layer\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(emb_dim, hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop_out),\n",
        "            nn.Linear(hid_dim, emb_dim)\n",
        "        )\n",
        "\n",
        "    # Forward pass of the attention block\n",
        "    def forward(self, x):\n",
        "        # x is the input tensor\n",
        "        # x has shape (batch_size, emb_dim, width, height)\n",
        "\n",
        "        # Apply normalization\n",
        "        x = self.layer_pre_norm(x)\n",
        "\n",
        "        # Apply the multi head attention layer\n",
        "        atten_out, attention = self.multi_head_atten(x, x, x)\n",
        "\n",
        "        # Add the input tensor and the output tensor\n",
        "        x = x + atten_out\n",
        "\n",
        "        # Apply normalization\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # Apply the feed forward layer\n",
        "        x = self.feed_forward(x)\n",
        "\n",
        "        # Add the input tensor and the output tensor\n",
        "        x = x + atten_out\n",
        "\n",
        "        # Return the output tensor and the attention tensor\n",
        "        return x, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IaOY2IBvisES"
      },
      "outputs": [],
      "source": [
        "# Define the Patch Layer\n",
        "class PatchLayer(nn.Module):\n",
        "    # Patch Layer Initialization\n",
        "    def __init__(self, in_chan, emb_dim, patch_size):\n",
        "        # in_chan is the input channel of the input tensor\n",
        "        # emb_dim is the embedding dimension\n",
        "        # patch_size is the size of the patch\n",
        "\n",
        "        # Call the parent class constructor\n",
        "        super(PatchLayer, self).__init__()\n",
        "\n",
        "        # Initialize all required parameters\n",
        "        self.in_chan = in_chan\n",
        "        self.emb_dim = emb_dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Initialize the patch layer\n",
        "        self.layer_patch = nn.Conv2d(in_chan, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    # Forward pass of the patch layer\n",
        "    def forward(self, x):\n",
        "        # x is the input tensor\n",
        "        # x has shape (batch_size, in_chan, width, height)\n",
        "        # batch_size is the number of samples in the batch\n",
        "        # in_chan is the input channel of the input tensor\n",
        "        # width is the width of the input tensor\n",
        "        # height is the height of the input tensor\n",
        "\n",
        "        # Compute the output tensor\n",
        "        out = self.layer_patch(x)\n",
        "\n",
        "        # Maintain the spatial dimensions\n",
        "        out = out.flatten(2).transpose(1, 2)\n",
        "\n",
        "        # Return the output tensor\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JTp5_NVWisET"
      },
      "outputs": [],
      "source": [
        "# Define the Vision Transformer\n",
        "class VisionTransformer(nn.Module):\n",
        "    # Vision Transformer Initialization\n",
        "    def __init__(self, img_size, in_chan, emb_dim, hid_dim, patch_size, num_layers, num_classes, num_heads = 8, drop_out = 0.0):\n",
        "        # in_chan is the input channel of the input tensor\n",
        "        # emb_dim is the embedding dimension\n",
        "        # hid_dim is the hidden dimension of the input tensor\n",
        "        # patch_size is the size of the patch\n",
        "        # num_layers is the number of layers in the vision transformer\n",
        "        # num_classes is the number of classes\n",
        "        # num_heads is the number of heads in the multi head attention layer\n",
        "        # drop_out is the drop out rate\n",
        "\n",
        "        # Call the parent class constructor\n",
        "        super(VisionTransformer, self).__init__()\n",
        "\n",
        "        # If the image size is not divisible by the patch size, raise an error\n",
        "        if img_size % patch_size != 0:\n",
        "            raise ValueError(\"Image size must be divisible by patch size\")\n",
        "\n",
        "        # Calculate the number of patches\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # Initialize the patch layer\n",
        "        self.patch_layer = PatchLayer(in_chan, emb_dim, patch_size)\n",
        "\n",
        "        # Initialize the attention blocks\n",
        "        self.atten_blocks = nn.ModuleList([AttenBlock(emb_dim, hid_dim, num_heads, drop_out) for _ in range(num_layers)])\n",
        "\n",
        "        # Initialize the layer normalization\n",
        "        self.layer_norm = nn.LayerNorm(emb_dim)\n",
        "\n",
        "        # Initialize the output layer\n",
        "        self.layer_out = nn.Linear(emb_dim, num_classes)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_enc = nn.Parameter(torch.randn(1, self.num_patches+1, emb_dim))\n",
        "        self.class_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
        "\n",
        "    # Forward pass of the vision transformer\n",
        "    def forward(self, x):\n",
        "        # x is the input tensor\n",
        "        # x has shape (batch_size, in_chan, width, height)\n",
        "        # batch_size is the number of samples in the batch\n",
        "        # in_chan is the input channel of the input tensor\n",
        "        # width is the width of the input tensor\n",
        "        # height is the height of the input tensor\n",
        "\n",
        "        # Compute the number of samples in the batch\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Apply the patch layer\n",
        "        x = self.patch_layer(x)\n",
        "\n",
        "        # Add the class token\n",
        "        cls_token = self.class_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "        # Add the positional encoding\n",
        "        x = x + self.pos_enc\n",
        "\n",
        "        # Apply the attention blocks\n",
        "        for atten_block in self.atten_blocks:\n",
        "            x, attention = atten_block(x)\n",
        "\n",
        "        # Apply normalization\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # Apply the output layer\n",
        "        x = self.layer_out(x[:, 0])\n",
        "\n",
        "        # Return the output tensor\n",
        "        return x\n",
        "\n",
        "    # Run the model\n",
        "    def run_model(self, criterion, optimizer, num_epoch, train_loader):\n",
        "        # criterion is the loss function\n",
        "        # optimizer is the optimizer\n",
        "        # num_epoch is the number of epochs\n",
        "        # train_loader is the training data loader\n",
        "        # logs is a boolean variable to print logs\n",
        "\n",
        "        # For each epoch\n",
        "        for epoch in range(num_epoch):\n",
        "            # Set the model to training mode\n",
        "            self.train()\n",
        "\n",
        "            # Find the batch size\n",
        "            batch_size = train_loader.batch_size\n",
        "\n",
        "            for data in tqdm(train_loader, total = len(train_loader)):\n",
        "\n",
        "                # Get the inputs and labels\n",
        "                inputs = data[0]\n",
        "                labels = data[1]\n",
        "\n",
        "                # If number of samples in the batch is less than the batch size, continue\n",
        "                if inputs.size()[0] != batch_size:\n",
        "                    continue\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self(inputs.to(device))\n",
        "\n",
        "                # Compute the loss\n",
        "                loss = criterion(outputs, labels.to(device))\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "\n",
        "                # Optimize\n",
        "                optimizer.step()\n",
        "\n",
        "    # Test the model\n",
        "    def test_model(self, test_loader):\n",
        "        corr = 0\n",
        "        tot = 0\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in tqdm(test_loader, total = len(test_loader)):\n",
        "                images = data[0]\n",
        "                labels = data[1]\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = self(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                tot += labels.size(0)\n",
        "                corr += (predicted == labels).sum().item()\n",
        "        acc = corr/tot\n",
        "        return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBx-M9k0isET",
        "outputId": "65955652-8e82-4bfc-9176-e9fbbd6e8946"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [00:19<00:00, 39.10it/s]\n",
            "100%|██████████| 782/782 [00:18<00:00, 41.26it/s]\n",
            "100%|██████████| 782/782 [00:20<00:00, 37.96it/s]\n",
            "100%|██████████| 782/782 [00:19<00:00, 39.33it/s]\n",
            "100%|██████████| 782/782 [00:19<00:00, 40.57it/s]\n",
            "100%|██████████| 782/782 [00:19<00:00, 39.98it/s]\n",
            "100%|██████████| 782/782 [00:18<00:00, 41.57it/s]\n",
            "100%|██████████| 782/782 [00:19<00:00, 40.12it/s]\n",
            "100%|██████████| 782/782 [00:18<00:00, 41.33it/s]\n",
            "100%|██████████| 782/782 [00:19<00:00, 39.96it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 55.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.5794\n"
          ]
        }
      ],
      "source": [
        "# Define model parameters\n",
        "img_size = 32\n",
        "patch_size = 8\n",
        "in_chan = 3\n",
        "emb_dim = 64\n",
        "hid_dim = 128\n",
        "num_layers = 3\n",
        "num_classes = 10\n",
        "num_heads = 8\n",
        "\n",
        "# Initialize the vision transformer\n",
        "model = VisionTransformer(img_size, in_chan, emb_dim, hid_dim, patch_size, num_layers, num_classes, num_heads)\n",
        "model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "if sgd:\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Run the model\n",
        "model.run_model(criterion, optimizer, number_of_epoch, train_loader)\n",
        "\n",
        "# Test the model\n",
        "acc = model.test_model(test_loader)\n",
        "\n",
        "print(\"Accuracy: \", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, for 10 epochs with almost the same number of parameters, we can compare the performance of the two models.\n",
        "\n",
        "We can see that the Transformer model is able to achieve a better accuracy than the CNN model in the same number of epochs.\n",
        "Also, the Transformer model is able to train a lot faster than the CNN model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
